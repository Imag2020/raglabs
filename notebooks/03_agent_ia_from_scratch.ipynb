{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2675c305-951d-46a5-88e5-e52499c1048c",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\"> 3. AI Agent from scratch </span>\n",
    "\n",
    "#### Now let's use our MCP tools implementation to build an LLM loop for an AI agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b925fddc-64c3-4045-804c-a426d4ed2f03",
   "metadata": {},
   "source": [
    "#### First lt's test our MCP tool without LLM call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8c041a-431f-4ce3-83bf-5d848a4bcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import ollama\n",
    "import os\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import List, Dict, Any, Optional, Set\n",
    "import time\n",
    "import traceback\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb530d-39c3-494f-967a-1ecae4cb2f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f72c15-11a4-4a0d-85eb-43c0a97fc900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPClient:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.server_config = self.load_mcp_servers_config()\n",
    "        self.process = None\n",
    "        self.request_id = 0\n",
    "        self.server_capabilities = None\n",
    "    \n",
    "    def load_mcp_servers_config(self):\n",
    "        try:\n",
    "            with open(\"mcp_servers.json\", \"r\") as file:\n",
    "                return json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            return {\n",
    "                'playwright': {\n",
    "                    'command': 'npx',\n",
    "                    'args': ['-y', '@executeautomation/playwright-mcp-server'],\n",
    "                    'description': 'Tools for internet navigation'\n",
    "                },\n",
    "                'filesystem': {\n",
    "                    'command': 'npx',\n",
    "                    'args': ['-y', '@modelcontextprotocol/server-filesystem', '/tmp'],\n",
    "                    'description': 'Tools for file system actions'\n",
    "                },\n",
    "                'github': {\n",
    "                    'command': 'npx',\n",
    "                    'args': ['-y', '@modelcontextprotocol/server-github'],\n",
    "                    'env': {'GITHUB_PERSONAL_ACCESS_TOKEN': 'your_token_here'}\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    async def start_server(self, server_name: str):\n",
    "        config = self.server_config\n",
    "        if \"mcpServers\" in config:\n",
    "            config = config[\"mcpServers\"]\n",
    "        \n",
    "        if server_name not in config:\n",
    "            raise ValueError(f\"Serveur {server_name} non trouv√© dans la configuration\")\n",
    "    \n",
    "        server_config = config[server_name]\n",
    "        \n",
    "        command = [server_config[\"command\"]] + server_config[\"args\"]\n",
    "        env = os.environ.copy()\n",
    "        if \"env\" in server_config:\n",
    "            env.update(server_config[\"env\"])\n",
    "        \n",
    "        print(f\"‚ñ∂Ô∏è D√©marrage du serveur MCP: {' '.join(command)}\")\n",
    "        \n",
    "        try:\n",
    "            self.process = await asyncio.create_subprocess_exec(\n",
    "                *command,\n",
    "                stdin=asyncio.subprocess.PIPE,\n",
    "                stdout=asyncio.subprocess.PIPE,\n",
    "                stderr=asyncio.subprocess.PIPE,\n",
    "                env=env\n",
    "            )\n",
    "            \n",
    "            async def log_stderr():\n",
    "                async for line in self.process.stderr:\n",
    "                    print(f\"   [MCP stderr]: {line.decode().strip()}\", flush=True)\n",
    "            asyncio.create_task(log_stderr())\n",
    "\n",
    "            await asyncio.sleep(3) \n",
    "            if self.process.returncode is not None:\n",
    "                raise RuntimeError(f\"Le serveur {server_name} s'est arr√™t√© pr√©matur√©ment avec le code {self.process.returncode}\")\n",
    "\n",
    "            print(f\"‚úÖ Serveur MCP '{server_name}' d√©marr√© (PID: {self.process.pid})\")\n",
    "            return self.process\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå √âchec du d√©marrage du serveur {server_name}: {e}\")\n",
    "            if self.process and self.process.returncode is None:\n",
    "                self.process.terminate()\n",
    "                await self.process.wait()\n",
    "            raise\n",
    "    \n",
    "    async def send_request(self, method: str, params: Dict = None):\n",
    "        if not self.process or self.process.returncode is not None:\n",
    "            raise RuntimeError(\"Serveur MCP non d√©marr√© ou arr√™t√©.\")\n",
    "        \n",
    "        self.request_id += 1\n",
    "        request = { \"jsonrpc\": \"2.0\", \"id\": self.request_id, \"method\": method, \"params\": params or {} }\n",
    "        \n",
    "        request_json = json.dumps(request) + \"\\n\"\n",
    "        self.process.stdin.write(request_json.encode())\n",
    "        await self.process.stdin.drain()\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                response_line = await asyncio.wait_for(self.process.stdout.readline(), timeout=20.0)\n",
    "                if not response_line: continue\n",
    "                response_data = json.loads(response_line)\n",
    "                if 'method' in response_data and response_data['method'].startswith('$/'): continue\n",
    "                if 'id' in response_data and response_data['id'] == self.request_id: return response_data\n",
    "            except asyncio.TimeoutError:\n",
    "                print(f\"‚è∞ Timeout en attente de la r√©ponse pour la requ√™te {self.request_id}\")\n",
    "                return {\"jsonrpc\": \"2.0\", \"id\": self.request_id, \"error\": {\"code\": -32603, \"message\": \"Request timed out\"}}\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Erreur de d√©codage JSON pour la ligne: {response_line.decode()}\")\n",
    "                continue\n",
    "    \n",
    "    async def initialize(self):\n",
    "        print(\"‚ñ∂Ô∏è Initialisation du serveur MCP...\")\n",
    "        init_response = await self.send_request(\"initialize\", {\n",
    "            \"protocolVersion\": \"2024-11-05\", \n",
    "            \"processId\": os.getpid(),\n",
    "            \"clientInfo\": {\"name\": \"ImprovedAgentSystem\", \"version\": \"0.3\"},\n",
    "            \"capabilities\": {}\n",
    "        })\n",
    "        if \"error\" in init_response:\n",
    "            raise RuntimeError(f\"√âchec de l'initialisation: {init_response['error']}\")\n",
    "        \n",
    "        self.server_capabilities = init_response.get(\"result\", {}).get(\"capabilities\", {})\n",
    "        print(\"‚úÖ Serveur MCP initialis√© avec succ√®s.\")\n",
    "        await self.send_request(\"initialized\")\n",
    "        return init_response\n",
    "\n",
    "    async def list_tools(self) -> List[Dict]: # CORRIG√â : Retourne seulement la liste.\n",
    "        print(\"üîç R√©cup√©ration de la liste des outils...\")\n",
    "        response = await self.send_request(\"tools/list\")\n",
    "        if \"error\" in response:\n",
    "            raise RuntimeError(f\"√âchec de la r√©cup√©ration des outils: {response['error']}\")\n",
    "        tools = response.get(\"result\", {}).get(\"tools\", [])\n",
    "        print(f\"  -> Trouv√© {len(tools)} outils.\")\n",
    "        return tools\n",
    "\n",
    "    async def call_tool(self, tool_name: str, arguments: Dict) -> Dict:\n",
    "        print(f\"‚ö°Ô∏è Appel MCP: Outil='{tool_name}', Arguments={arguments}\")\n",
    "        response = await self.send_request(\"tools/call\", {\"name\": tool_name, \"arguments\": arguments})\n",
    "        if \"error\" in response:\n",
    "            print(f\"‚ùå Erreur lors de l'appel de l'outil: {response['error']}\")\n",
    "            return {\"success\": False, \"error\": response['error']}\n",
    "        result = {\"success\": True, \"result\": response.get('result')}\n",
    "        print(f\"‚úîÔ∏è R√©ponse de l'outil: {json.dumps(result, indent=2, ensure_ascii=False)}\")\n",
    "        return result\n",
    "\n",
    "    async def stop_server(self):\n",
    "        if self.process and self.process.returncode is None:\n",
    "            print(\"‚ñ∂Ô∏è Arr√™t du serveur MCP...\")\n",
    "            await self.send_request(\"shutdown\")\n",
    "            await self.send_request(\"exit\")\n",
    "            try: await asyncio.wait_for(self.process.wait(), timeout=5.0)\n",
    "            except asyncio.TimeoutError: self.process.kill()\n",
    "            print(\"‚úÖ Serveur MCP arr√™t√©.\")\n",
    "            self.process = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af484a0d-cb46-425c-b93e-c2e7e3b670a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MCPClient()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65bb396c-728d-4371-ac07-9af1277d4685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è D√©marrage du serveur MCP: npx -y @modelcontextprotocol/server-filesystem /tmp\n",
      "   [MCP stderr]: Secure MCP Filesystem Server running on stdio\n",
      "   [MCP stderr]: Allowed directories: [ \u001b[32m'/tmp'\u001b[39m ]\n",
      "‚úÖ Serveur MCP 'filesystem' d√©marr√© (PID: 58027)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Process 58027>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.start_server('filesystem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c554e1ab-708a-4e02-a3fa-a2ae8f5d23e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è Initialisation du serveur MCP...\n",
      "‚úÖ Serveur MCP initialis√© avec succ√®s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'result': {'protocolVersion': '2024-11-05',\n",
       "  'capabilities': {'tools': {}},\n",
       "  'serverInfo': {'name': 'secure-filesystem-server', 'version': '0.2.0'}},\n",
       " 'jsonrpc': '2.0',\n",
       " 'id': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c25ea7f-a938-432a-af57-8495cc3436c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç R√©cup√©ration de la liste des outils...\n",
      "  -> Trouv√© 12 outils.\n"
     ]
    }
   ],
   "source": [
    "tools = await client.list_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0a39faa-e4f7-4d0a-856c-bb7da09bc46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'read_file',\n",
       "  'description': \"Read the complete contents of a file from the file system. Handles various text encodings and provides detailed error messages if the file cannot be read. Use this tool when you need to examine the contents of a single file. Use the 'head' parameter to read only the first N lines of a file, or the 'tail' parameter to read only the last N lines of a file. Only works within allowed directories.\",\n",
       "  'inputSchema': {'type': 'object',\n",
       "   'properties': {'path': {'type': 'string'},\n",
       "    'tail': {'type': 'number',\n",
       "     'description': 'If provided, returns only the last N lines of the file'},\n",
       "    'head': {'type': 'number',\n",
       "     'description': 'If provided, returns only the first N lines of the file'}},\n",
       "   'required': ['path'],\n",
       "   'additionalProperties': False,\n",
       "   '$schema': 'http://json-schema.org/draft-07/schema#'}},\n",
       " {'name': 'read_multiple_files',\n",
       "  'description': \"Read the contents of multiple files simultaneously. This is more efficient than reading files one by one when you need to analyze or compare multiple files. Each file's content is returned with its path as a reference. Failed reads for individual files won't stop the entire operation. Only works within allowed directories.\",\n",
       "  'inputSchema': {'type': 'object',\n",
       "   'properties': {'paths': {'type': 'array', 'items': {'type': 'string'}}},\n",
       "   'required': ['paths'],\n",
       "   'additionalProperties': False,\n",
       "   '$schema': 'http://json-schema.org/draft-07/schema#'}},\n",
       " {'name': 'write_file',\n",
       "  'description': 'Create a new file or completely overwrite an existing file with new content. Use with caution as it will overwrite existing files without warning. Handles text content with proper encoding. Only works within allowed directories.',\n",
       "  'inputSchema': {'type': 'object',\n",
       "   'properties': {'path': {'type': 'string'}, 'content': {'type': 'string'}},\n",
       "   'required': ['path', 'content'],\n",
       "   'additionalProperties': False,\n",
       "   '$schema': 'http://json-schema.org/draft-07/schema#'}},\n",
       " {'name': 'edit_file',\n",
       "  'description': 'Make line-based edits to a text file. Each edit replaces exact line sequences with new content. Returns a git-style diff showing the changes made. Only works within allowed directories.',\n",
       "  'inputSchema': {'type': 'object',\n",
       "   'properties': {'path': {'type': 'string'},\n",
       "    'edits': {'type': 'array',\n",
       "     'items': {'type': 'object',\n",
       "      'properties': {'oldText': {'type': 'string',\n",
       "        'description': 'Text to search for - must match exactly'},\n",
       "       'newText': {'type': 'string', 'description': 'Text to replace with'}},\n",
       "      'required': ['oldText', 'newText'],\n",
       "      'additionalProperties': False}},\n",
       "    'dryRun': {'type': 'boolean',\n",
       "     'default': False,\n",
       "     'description': 'Preview changes using git-style diff format'}},\n",
       "   'required': ['path', 'edits'],\n",
       "   'additionalProperties': False,\n",
       "   '$schema': 'http://json-schema.org/draft-07/schema#'}},\n",
       " {'name': 'create_directory',\n",
       "  'description': 'Create a new directory or ensure a directory exists. Can create multiple nested directories in one operation. If the directory already exists, this operation will succeed silently. Perfect for setting up directory structures for projects or ensuring required paths exist. Only works within allowed directories.',\n",
       "  'inputSchema': {'type': 'object',\n",
       "   'properties': {'path': {'type': 'string'}},\n",
       "   'required': ['path'],\n",
       "   'additionalProperties': False,\n",
       "   '$schema': 'http://json-schema.org/draft-07/schema#'}},\n",
       " {'name': 'list_directory',\n",
       "  'description': 'Get a detailed listing of all files and directories in a specified path. Results clearly distinguish between files and directories with [FILE] and [DIR] prefixes. This tool is essential for understanding directory structure and finding specific files within a directory. Only works within allowed directories.',\n",
       "  'inputSchema': {'type': 'object',\n",
       "   'properties': {'path': {'type': 'string'}},\n",
       "   'required': ['path'],\n",
       "   'additionalProperties': False,\n",
       "   '$schema': 'http://json-schema.org/draft-07/schema#'}},\n",
       " {'name': 'list_directory_with_sizes',\n",
       "  'description': 'Get a detailed listing of all files and directories in a specified path, including sizes. Results clearly distinguish between files and directories with [FILE] and [DIR] prefixes. This tool is useful for understanding directory structure and finding specific files within a directory. Only works within allowed directories.',\n",
       "  'inputSchema': {'type': 'object',\n",
       "   'properties': {'path': {'type': 'string'},\n",
       "    'sortBy': {'type': 'string',\n",
       "     'enum': ['name', 'size'],\n",
       "     'default': 'name',\n",
       "     'description': 'Sort entries by name or size'}},\n",
       "   'required': ['path'],\n",
       "   'additionalProperties': False,\n",
       "   '$schema': 'http://json-schema.org/draft-07/schema#'}},\n",
       " {'name': 'directory_tree',\n",
       "  'description': \"Get a recursive tree view of files and directories as a JSON structure. Each entry includes 'name', 'type' (file/directory), and 'children' for directories. Files have no children array, while directories always have a children array (which may be empty). The output is formatted with 2-space indentation for readability. Only works within allowed directories.\",\n",
       "  'inputSchema': {'type': 'object',\n",
       "   'properties': {'path': {'type': 'string'}},\n",
       "   'required': ['path'],\n",
       "   'additionalProperties': False,\n",
       "   '$schema': 'http://json-schema.org/draft-07/schema#'}},\n",
       " {'name': 'move_file',\n",
       "  'description': 'Move or rename files and directories. Can move files between directories and rename them in a single operation. If the destination exists, the operation will fail. Works across different directories and can be used for simple renaming within the same directory. Both source and destination must be within allowed directories.',\n",
       "  'inputSchema': {'type': 'object',\n",
       "   'properties': {'source': {'type': 'string'},\n",
       "    'destination': {'type': 'string'}},\n",
       "   'required': ['source', 'destination'],\n",
       "   'additionalProperties': False,\n",
       "   '$schema': 'http://json-schema.org/draft-07/schema#'}},\n",
       " {'name': 'search_files',\n",
       "  'description': \"Recursively search for files and directories matching a pattern. Searches through all subdirectories from the starting path. The search is case-insensitive and matches partial names. Returns full paths to all matching items. Great for finding files when you don't know their exact location. Only searches within allowed directories.\",\n",
       "  'inputSchema': {'type': 'object',\n",
       "   'properties': {'path': {'type': 'string'},\n",
       "    'pattern': {'type': 'string'},\n",
       "    'excludePatterns': {'type': 'array',\n",
       "     'items': {'type': 'string'},\n",
       "     'default': []}},\n",
       "   'required': ['path', 'pattern'],\n",
       "   'additionalProperties': False,\n",
       "   '$schema': 'http://json-schema.org/draft-07/schema#'}},\n",
       " {'name': 'get_file_info',\n",
       "  'description': 'Retrieve detailed metadata about a file or directory. Returns comprehensive information including size, creation time, last modified time, permissions, and type. This tool is perfect for understanding file characteristics without reading the actual content. Only works within allowed directories.',\n",
       "  'inputSchema': {'type': 'object',\n",
       "   'properties': {'path': {'type': 'string'}},\n",
       "   'required': ['path'],\n",
       "   'additionalProperties': False,\n",
       "   '$schema': 'http://json-schema.org/draft-07/schema#'}},\n",
       " {'name': 'list_allowed_directories',\n",
       "  'description': 'Returns the list of directories that this server is allowed to access. Use this to understand which directories are available before trying to access files.',\n",
       "  'inputSchema': {'type': 'object', 'properties': {}, 'required': []}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f46f767d-523a-4bfc-8b94-c63c5e9ab883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è D√©marrage du serveur MCP: npx -y @modelcontextprotocol/server-filesystem /tmp\n",
      "   [MCP stderr]: Secure MCP Filesystem Server running on stdio\n",
      "   [MCP stderr]: Allowed directories: [ \u001b[32m'/tmp'\u001b[39m ]\n",
      "‚úÖ Serveur MCP 'filesystem' d√©marr√© (PID: 58107)\n",
      "‚ñ∂Ô∏è Initialisation du serveur MCP...\n",
      "‚úÖ Serveur MCP initialis√© avec succ√®s.\n",
      "User: Create a file named example.txt with content Hello, this is a test file!\n",
      "‚ö°Ô∏è Appel MCP: Outil='write_file', Arguments={'path': '/tmp/example.txt', 'content': 'Hello, this is a test file!'}\n",
      "‚úîÔ∏è R√©ponse de l'outil: {\n",
      "  \"success\": true,\n",
      "  \"result\": {\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"Successfully wrote to /tmp/example.txt\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Agent: File example.txt created successfully with content: Hello, this is a test file!\n",
      "üîç R√©cup√©ration de la liste des outils...\n",
      "  -> Trouv√© 12 outils.\n",
      "Available tools: ['read_file', 'read_multiple_files', 'write_file', 'edit_file', 'create_directory', 'list_directory', 'list_directory_with_sizes', 'directory_tree', 'move_file', 'search_files', 'get_file_info', 'list_allowed_directories']\n",
      "‚ñ∂Ô∏è Arr√™t du serveur MCP...\n",
      "‚úÖ Serveur MCP arr√™t√©.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import uuid\n",
    "import requests \n",
    "class SimpleAIAgent:\n",
    "    def __init__(self, mcp_client):\n",
    "        self.mcp_client = mcp_client\n",
    "        self.conversation_history = []\n",
    "\n",
    "    async def start(self):\n",
    "        \"\"\"Start the filesystem server and initialize it.\"\"\"\n",
    "        await self.mcp_client.start_server(\"filesystem\")\n",
    "        await self.mcp_client.initialize()\n",
    "\n",
    "    async def process_user_request(self, user_input: str) -> str:\n",
    "        \"\"\"Process a user request and return the response.\"\"\"\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Simple logic to detect file creation request\n",
    "        if \"create a file\" in user_input.lower():\n",
    "            try:\n",
    "                # Extract file name and content from user input\n",
    "                # This is a simplified parsing; in production, use more robust parsing\n",
    "                file_name = \"example.txt\"\n",
    "                content = user_input.split(\"with content\")[-1].strip() if \"with content\" in user_input.lower() else \"Default content\"\n",
    "                \n",
    "                # Call MCP filesystem tool to create file\n",
    "                result = await self.mcp_client.call_tool(\n",
    "                    \"write_file\",\n",
    "                    {\n",
    "                        \"path\": f\"/tmp/{file_name}\",\n",
    "                        \"content\": content\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if result[\"success\"]:\n",
    "                    response = f\"File {file_name} created successfully with content: {content}\"\n",
    "                else:\n",
    "                    response = f\"Failed to create file: {result['error']['message']}\"\n",
    "            except Exception as e:\n",
    "                response = f\"Error processing request: {str(e)}\"\n",
    "        else:\n",
    "            response = \"I can help create files. Please specify a file creation request, e.g., 'create a file named example.txt with content Hello World'\"\n",
    "\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return response\n",
    "\n",
    "    async def stop(self):\n",
    "        \"\"\"Stop the MCP server.\"\"\"\n",
    "        await self.mcp_client.stop_server()\n",
    "\n",
    "async def main():\n",
    "    # Initialize MCP client and agent\n",
    "    mcp_client = MCPClient()\n",
    "    agent = SimpleAIAgent(mcp_client)\n",
    "    \n",
    "    try:\n",
    "        # Start the agent\n",
    "        await agent.start()\n",
    "        \n",
    "        # Example user request\n",
    "        user_request = \"Create a file named example.txt with content Hello, this is a test file!\"\n",
    "        print(f\"User: {user_request}\")\n",
    "        response = await agent.process_user_request(user_request)\n",
    "        print(f\"Agent: {response}\")\n",
    "        \n",
    "        # List available tools for demonstration\n",
    "        tools = await mcp_client.list_tools()\n",
    "        print(f\"Available tools: {[tool['name'] for tool in tools]}\")\n",
    "        \n",
    "    finally:\n",
    "        # Ensure server is stopped\n",
    "        await agent.stop()\n",
    "\n",
    "# Modified execution for environments with a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check if there's an existing event loop\n",
    "        loop = asyncio.get_running_loop()\n",
    "        # If we're here, we're in a running event loop (e.g., Jupyter)\n",
    "        loop.create_task(main())\n",
    "    except RuntimeError:\n",
    "        # No running event loop, so we can use asyncio.run()\n",
    "        asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d45783-6259-4fb0-b927-042b4917852f",
   "metadata": {},
   "source": [
    "#### Now let's use our LLM, we need helper function to parse LLM input\n",
    "#### We  need only the json output without the thinking blocs or json tags\n",
    "#### By default we will use local LLM for this simple task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1366090-f3e1-4132-b02f-493b06a42ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_output(output: str) -> str:\n",
    "    # Supprime <think>...</think> au d√©but si pr√©sent\n",
    "    if output.startswith(\"<think>\"):\n",
    "        end_think = output.find(\"</think>\")\n",
    "        if end_think != -1:\n",
    "            output = output[end_think + len(\"</think>\"):]\n",
    "    \n",
    "    # Supprime ```json au d√©but et ``` √† la fin\n",
    "    if output.startswith(\"```json\"):\n",
    "        output = output[len(\"```json\"):]  # Enl√®ve le d√©but\n",
    "    if output.endswith(\"```\"):\n",
    "        output = output[:-len(\"```\")]  # Enl√®ve la fin\n",
    "    \n",
    "    return output.strip()  # Nettoie les espaces √©ventuels\n",
    "\n",
    "def llm_ollama(prompt,model='gemma3:4b'):\n",
    "      \n",
    "    url = \"http://127.0.0.1:11434/api/generate\"\n",
    "    data = {\n",
    "    \"model\": model,\n",
    "    \"prompt\" : prompt,\n",
    "    \"stream\": False,\n",
    "  \n",
    "    }\n",
    "    response = requests.post(url, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['response']\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return response.text\n",
    "\n",
    "def llm(prompt,endpoint='ollama',model='default'):\n",
    "    \n",
    "\n",
    "    print(f\" _________ {endpoint} / {model}__________ {prompt} \")\n",
    "\n",
    "   \n",
    "\n",
    "    if endpoint == 'ollama':\n",
    "        if model == 'default':\n",
    "            ret=llm_ollama(prompt)\n",
    "        else:\n",
    "            ret=llm_ollama(prompt,model)\n",
    "            \n",
    "    elif endpoint == 'gemini':\n",
    "        if model == 'default':\n",
    "            ret=llm_gemini(prompt)\n",
    "        else:\n",
    "            ret=llm_gemini(prompt,model)\n",
    "            \n",
    "    elif endpoint =='deepseek':\n",
    "        if model == 'default':\n",
    "            ret=llm_deepseek(prompt)\n",
    "        else:\n",
    "            ret=llm_deepseek(prompt,model)\n",
    "\n",
    "    elif endpoint =='claude':\n",
    "        if model == 'default':\n",
    "            ret=llm_claude(prompt)\n",
    "        else:\n",
    "            ret=llm_claude(prompt,model)\n",
    "            \n",
    "    elif endpoint =='openai':\n",
    "        if model == 'default':\n",
    "            ret=llm_openai(prompt)\n",
    "        else:\n",
    "            ret=llm_openai(prompt,model)\n",
    "    \n",
    "    elif endpoint =='grok':\n",
    "        if model == 'default':\n",
    "            ret=llm_openai(prompt)\n",
    "        else:\n",
    "            ret=llm_grok(prompt,model)\n",
    "    \n",
    "       \n",
    "    print(\"\",f\"----------ret {endpoint}/{model}-------- \",ret,\" ----------- \",\"\")\n",
    "    return clean_llm_output(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f819246-224c-46a9-aa59-d22347ba729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _________ ollama / default__________ hi \n",
      " ----------ret ollama/default--------  Hi there! How's your day going so far? üòä \n",
      "\n",
      "Is there anything you'd like to chat about, or were you just saying hello?  -----------  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi there! How's your day going so far? üòä \\n\\nIs there anything you'd like to chat about, or were you just saying hello?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d59157e-a5ec-4c06-8574-13e2d9d7bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAIAgent:\n",
    "    def __init__(self, mcp_client):\n",
    "        self.mcp_client = mcp_client\n",
    "        self.conversation_history = []\n",
    "\n",
    "    async def start(self):\n",
    "        \"\"\"Start the filesystem server and initialize it.\"\"\"\n",
    "        await self.mcp_client.start_server(\"filesystem\")\n",
    "        await self.mcp_client.initialize()\n",
    "\n",
    "    async def process_user_request(self, user_input: str) -> str:\n",
    "        \"\"\"Process a user request using the LLM and return the response.\"\"\"\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Create a prompt for the LLM including conversation history\n",
    "        prompt = (\n",
    "            \"You are an AI assistant that can create files using a filesystem tool. \"\n",
    "            \"Based on the user input and conversation history, return a JSON object with the action to take. \"\n",
    "            \"For file creation, return: {'action': 'create_file', 'file_name': '<name>', 'content': '<content>'}. \"\n",
    "            \"For unrecognized requests, return: {'action': 'unknown', 'message': '<response>'}.\\n\\n\"\n",
    "            f\"Conversation history: {json.dumps(self.conversation_history, indent=2)}\\n\"\n",
    "            f\"User input: {user_input}\"\n",
    "        )\n",
    "        \n",
    "        # Call the LLM\n",
    "        llm_response = llm(prompt)\n",
    "        \n",
    "        # Process LLM response\n",
    "        if llm_response.get(\"action\") == \"create_file\":\n",
    "            try:\n",
    "                file_name = llm_response.get(\"file_name\", \"example3.txt\")\n",
    "                content = llm_response.get(\"content\", \"Default content\")\n",
    "                \n",
    "                # Call MCP filesystem tool to create file\n",
    "                result = await self.mcp_client.call_tool(\n",
    "                    \"write_file\",\n",
    "                    {\n",
    "                        \"path\": f\"/tmp/{file_name}\",\n",
    "                        \"content\": content\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if result[\"success\"]:\n",
    "                    response = f\"File {file_name} created successfully with content: {content}\"\n",
    "                else:\n",
    "                    response = f\"Failed to create file: {result['error']['message']}\"\n",
    "            except Exception as e:\n",
    "                response = f\"Error processing request: {str(e)}\"\n",
    "        else:\n",
    "            response = llm_response.get(\"message\", \"I can help create files. Please specify a file creation request, e.g., 'create a file named example.txt with content Hello World'\")\n",
    "\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return response\n",
    "\n",
    "    async def stop(self):\n",
    "        \"\"\"Stop the MCP server.\"\"\"\n",
    "        await self.mcp_client.stop_server()\n",
    "\n",
    "async def main():\n",
    "    # Initialize MCP client and agent\n",
    "    mcp_client = MCPClient()\n",
    "    agent = SimpleAIAgent(mcp_client)\n",
    "    \n",
    "    try:\n",
    "        # Start the agent\n",
    "        await agent.start()\n",
    "        \n",
    "        # Example user request\n",
    "        user_request = \"Create a file named example2.txt with content Hi, this is a test file!\"\n",
    "        print(f\"User: {user_request}\")\n",
    "        response = await agent.process_user_request(user_request)\n",
    "        print(f\"Agent: {response}\")\n",
    "        \n",
    "        # List available tools for demonstration\n",
    "        tools = await mcp_client.list_tools()\n",
    "        print(f\"Available tools: {[tool['name'] for tool in tools]}\")\n",
    "        \n",
    "    finally:\n",
    "        # Ensure server is stopped\n",
    "        await agent.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a713ec9-d72b-478b-be3a-1a7f461473ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è D√©marrage du serveur MCP: npx -y @modelcontextprotocol/server-filesystem /tmp\n",
      "   [MCP stderr]: Secure MCP Filesystem Server running on stdio\n",
      "   [MCP stderr]: Allowed directories: [ \u001b[32m'/tmp'\u001b[39m ]\n",
      "‚úÖ Serveur MCP 'filesystem' d√©marr√© (PID: 58201)\n",
      "‚ñ∂Ô∏è Initialisation du serveur MCP...\n",
      "‚úÖ Serveur MCP initialis√© avec succ√®s.\n",
      "User: Create a file named example2.txt with content Hi, this is a test file!\n",
      " _________ ollama / default__________ You are an AI assistant that can create files using a filesystem tool. Based on the user input and conversation history, return a JSON object with the action to take. For file creation, return: {'action': 'create_file', 'file_name': '<name>', 'content': '<content>'}. For unrecognized requests, return: {'action': 'unknown', 'message': '<response>'}.\n",
      "\n",
      "Conversation history: [\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Create a file named example2.txt with content Hi, this is a test file!\"\n",
      "  }\n",
      "]\n",
      "User input: Create a file named example2.txt with content Hi, this is a test file! \n",
      " ----------ret ollama/default--------  ```json\n",
      "{\n",
      "  \"action\": \"create_file\",\n",
      "  \"file_name\": \"example2.txt\",\n",
      "  \"content\": \"Hi, this is a test file!\"\n",
      "}\n",
      "```  -----------  \n",
      "‚ñ∂Ô∏è Arr√™t du serveur MCP...\n",
      "‚úÖ Serveur MCP arr√™t√©.\n"
     ]
    }
   ],
   "source": [
    "# Modified execution for environments with a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check if there's an existing event loop\n",
    "        loop = asyncio.get_running_loop()\n",
    "        # If we're here, we're in a running event loop (e.g., Jupyter)\n",
    "        loop.create_task(main())\n",
    "    except RuntimeError:\n",
    "        # No running event loop, so we can use asyncio.run()\n",
    "        asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd3b81-8e43-4f03-ba95-8a2c9e7045d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f14ad2-6bfa-4416-9d06-4806755bdc61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
